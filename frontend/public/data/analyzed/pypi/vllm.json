{
  "ecosystem": "pypi",
  "name": "vllm",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "version": "0.13.0",
  "homepage": "https://pypi.org/project/vllm/",
  "repository": {
    "platform": "github",
    "owner": "vllm-project",
    "repo": "vllm",
    "subpath": null
  },
  "install_count_30d": 2783654,
  "data_availability": "available",
  "unavailable_reason": null,
  "scores": {
    "overall": 80.4,
    "grade": "B",
    "percentile": null,
    "risk_tier": "prohibited",
    "update_urgency": "critical",
    "confidence": "medium",
    "confidence_factors": [
      "No LLM assessment available"
    ],
    "project_age_band": "established",
    "security": {
      "score": 50.0,
      "weight": 30,
      "key_factors": []
    },
    "maintenance": {
      "score": 100.0,
      "weight": 25,
      "key_factors": []
    },
    "community": {
      "score": 100.0,
      "weight": 15,
      "key_factors": []
    },
    "bus_factor": {
      "score": 100.0,
      "weight": 10,
      "key_factors": []
    },
    "documentation": {
      "score": 63.0,
      "weight": 10,
      "key_factors": []
    },
    "stability": {
      "score": 91.0,
      "weight": 10,
      "key_factors": []
    }
  },
  "github_data": {
    "repo": {
      "owner": "vllm-project",
      "name": "vllm",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "stars": 67276,
      "forks": 12517,
      "open_issues": 3047,
      "watchers": 67276,
      "created_at": "2023-02-09T11:23:20Z",
      "updated_at": "2026-01-11T20:44:02Z",
      "pushed_at": "2026-01-11T15:11:11Z",
      "default_branch": "main",
      "license": "Apache-2.0",
      "language": "Python",
      "topics": [
        "amd",
        "blackwell",
        "cuda",
        "deepseek",
        "deepseek-v3",
        "gpt",
        "gpt-oss",
        "inference",
        "kimi",
        "llama",
        "llm",
        "llm-serving",
        "model-serving",
        "moe",
        "openai",
        "pytorch",
        "qwen",
        "qwen3",
        "tpu",
        "transformer"
      ],
      "is_archived": false,
      "is_fork": false,
      "has_discussions": true,
      "is_deprecated": false
    },
    "contributors": {
      "total_contributors": 459,
      "active_contributors_6mo": 139,
      "top_contributor_pct": 6.9,
      "contributors_over_5pct": 2,
      "contributors_prev_6mo": 0,
      "contributor_trend": "growing",
      "first_time_contributors_6mo": 139,
      "contributor_entropy": 7.09
    },
    "commits": {
      "last_commit_date": "2026-01-11T15:11:11Z",
      "commits_last_6mo": 300,
      "commits_last_year": 300
    },
    "issues": {
      "open_issues": 120,
      "closed_issues_6mo": 34,
      "avg_response_time_hours": 6.2,
      "avg_close_time_hours": 15.8,
      "good_first_issue_count": 1,
      "regression_issue_count": 0
    },
    "prs": {
      "open_prs": 300,
      "merged_prs_6mo": 244,
      "closed_prs_6mo": 300,
      "stale_prs": 0,
      "avg_merge_time_hours": null
    },
    "releases": {
      "total_releases": 79,
      "releases_last_year": 33,
      "last_release_date": "2025-12-19T03:02:22Z",
      "latest_version": "v0.13.0",
      "has_signed_releases": false,
      "prerelease_ratio": 0.11
    },
    "security": {
      "has_security_md": true,
      "has_security_policy": false,
      "signed_commits_pct": 100.0,
      "has_dependabot": true,
      "has_codeql": false,
      "has_security_ci": true,
      "has_snyk": false,
      "has_renovate": false,
      "has_trivy": false,
      "has_semgrep": false,
      "slsa_level": null,
      "has_sigstore": false,
      "has_sbom": false,
      "has_reproducible_builds": false,
      "known_cves": 42,
      "vulnerable_deps": 0,
      "cve_history": {
        "total_cves": 42,
        "cves": [
          {
            "id": "GHSA-hjq4-87xh-g4fv",
            "summary": "vLLM Allows Remote Code Execution via PyNcclPipe Communication Service",
            "severity": "CRITICAL",
            "cvss_score": null,
            "published_date": "2025-05-20T18:04:30Z",
            "fixed_version": "0.8.5",
            "patch_release_date": "2025-04-28T21:13:09Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-hjq4-87xh-g4fv",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-47277",
              "https://github.com/vllm-project/vllm/pull/15988",
              "https://github.com/vllm-project/vllm/commit/0d6e187e88874c39cda7409cf673f9e6546893e7",
              "https://docs.vllm.ai/en/latest/deployment/security.html"
            ]
          },
          {
            "id": "GHSA-hj4w-hm2g-p6w5",
            "summary": "vLLM Vulnerable to Remote Code Execution via Mooncake Integration",
            "severity": "CRITICAL",
            "cvss_score": null,
            "published_date": "2025-04-29T14:52:29Z",
            "fixed_version": "0.8.5",
            "patch_release_date": "2025-04-28T21:13:09Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-hj4w-hm2g-p6w5",
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-x3m8-f7g5-qhm7",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-32444",
              "https://github.com/vllm-project/vllm/commit/a5450f11c95847cf51a17207af9a3ca5ab569b2c",
              "https://github.com/pypa/advisory-database/tree/main/vulns/vllm/PYSEC-2025-42.yaml"
            ]
          },
          {
            "id": "GHSA-ggpf-24jw-3fcw",
            "summary": "CVE-2025-24357 Malicious model remote code execution fix bypass with PyTorch < 2.6.0",
            "severity": "CRITICAL",
            "cvss_score": null,
            "published_date": "2025-04-23T02:26:06Z",
            "fixed_version": "0.8.0",
            "patch_release_date": "2025-03-18T17:52:20Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/pytorch/pytorch/security/advisories/GHSA-53q9-r3pm-6pq6",
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-ggpf-24jw-3fcw",
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-rh4j-5rhw-hr54",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-cj47-qj6g-x7r4",
            "summary": "vLLM allows Remote Code Execution by Pickle Deserialization via AsyncEngineRPCServer() RPC server entrypoints",
            "severity": "CRITICAL",
            "cvss_score": null,
            "published_date": "2025-03-20T12:32:50Z",
            "fixed_version": null,
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://nvd.nist.gov/vuln/detail/CVE-2024-9053",
              "https://github.com/vllm-project/vllm",
              "https://huntr.com/bounties/75a544f3-34a3-4da0-b5a3-1495cb031e09"
            ]
          },
          {
            "id": "GHSA-pgr7-mhp5-fgjp",
            "summary": "vLLM deserialization vulnerability in vllm.distributed.GroupCoordinator.recv_object",
            "severity": "CRITICAL",
            "cvss_score": null,
            "published_date": "2025-03-20T12:32:50Z",
            "fixed_version": null,
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://nvd.nist.gov/vuln/detail/CVE-2024-9052",
              "https://github.com/github/advisory-database/pull/5444",
              "https://github.com/vllm-project/vllm",
              "https://github.com/vllm-project/vllm/blob/32e7db25365415841ebc7c4215851743fbb1bad1/vllm/distributed/parallel_state.py#L480",
              "https://github.com/vllm-project/vllm/blob/v0.8.1/vllm/distributed/parallel_state.py#L457"
            ]
          },
          {
            "id": "GHSA-5vqr-wprc-cpp7",
            "summary": "vLLM Deserialization of Untrusted Data vulnerability",
            "severity": "CRITICAL",
            "cvss_score": null,
            "published_date": "2025-03-20T12:32:41Z",
            "fixed_version": null,
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://nvd.nist.gov/vuln/detail/CVE-2024-11041",
              "https://github.com/vllm-project/vllm",
              "https://github.com/vllm-project/vllm/blob/7193774b1ff8603ad5bf4598e5efba0d9a39b436/vllm/distributed/device_communicators/shm_broadcast.py#L441-L443",
              "https://huntr.com/bounties/00136195-11e0-4ad0-98d5-72db066e867f"
            ]
          },
          {
            "id": "GHSA-x3m8-f7g5-qhm7",
            "summary": "vLLM Allows Remote Code Execution via Mooncake Integration",
            "severity": "CRITICAL",
            "cvss_score": null,
            "published_date": "2025-03-19T15:55:58Z",
            "fixed_version": "0.8.0",
            "patch_release_date": "2025-03-18T17:52:20Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-x3m8-f7g5-qhm7",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-29783",
              "https://github.com/vllm-project/vllm/pull/14228",
              "https://github.com/vllm-project/vllm/commit/288ca110f68d23909728627d3100e5a8db820aa2",
              "https://github.com/pypa/advisory-database/tree/main/vulns/vllm/PYSEC-2025-63.yaml"
            ]
          },
          {
            "id": "GHSA-mcmc-2m55-j8jj",
            "summary": "vLLM introduced enhanced protection for CVE-2025-62164",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2026-01-08T21:47:43Z",
            "fixed_version": "0.13.0",
            "patch_release_date": "2025-12-19T03:02:22Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-mcmc-2m55-j8jj",
              "https://github.com/vllm-project/vllm/pull/30649",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-8fr4-5q9j-m8gm",
            "summary": "vLLM vulnerable to remote code execution via transformers_utils/get_config",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-12-02T17:34:16Z",
            "fixed_version": "0.11.1",
            "patch_release_date": "2025-11-18T23:03:42Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-8fr4-5q9j-m8gm",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-66448",
              "https://github.com/vllm-project/vllm/pull/28126",
              "https://github.com/vllm-project/vllm/commit/ffb08379d8870a1a81ba82b72797f196838d0c86",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-pmqf-x6x8-p7qw",
            "summary": "vLLM vulnerable to DoS with incorrect shape of multimodal embedding inputs",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-11-20T21:23:29Z",
            "fixed_version": "0.11.1",
            "patch_release_date": "2025-11-18T23:03:42Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-pmqf-x6x8-p7qw",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-62372",
              "https://github.com/vllm-project/vllm/pull/27204",
              "https://github.com/vllm-project/vllm/pull/6613",
              "https://github.com/vllm-project/vllm/commit/58fab50d82838d5014f4a14d991fdb9352c9c84b"
            ]
          },
          {
            "id": "GHSA-mrw7-hf4f-83pf",
            "summary": "vLLM deserialization vulnerability leading to DoS and potential RCE",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-11-20T20:59:34Z",
            "fixed_version": "0.11.1",
            "patch_release_date": "2025-11-18T23:03:42Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-mrw7-hf4f-83pf",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-62164",
              "https://github.com/vllm-project/vllm/pull/27204",
              "https://github.com/vllm-project/vllm/commit/58fab50d82838d5014f4a14d991fdb9352c9c84b",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-3f6c-7fw2-ppm4",
            "summary": "vLLM is vulnerable to Server-Side Request Forgery (SSRF) through `MediaConnector` class",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-10-07T22:14:15Z",
            "fixed_version": "0.11.0",
            "patch_release_date": "2025-10-02T19:17:04Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-3f6c-7fw2-ppm4",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-6242",
              "https://github.com/vllm-project/vllm/commit/9d9a2b77f19f68262d5e469c4e82c0f6365ad72d",
              "https://access.redhat.com/security/cve/CVE-2025-6242",
              "https://bugzilla.redhat.com/show_bug.cgi?id=2373716"
            ]
          },
          {
            "id": "GHSA-wr9h-g72x-mwhm",
            "summary": "vLLM is vulnerable to timing attack at bearer auth",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-10-07T17:24:47Z",
            "fixed_version": "0.11.0",
            "patch_release_date": "2025-10-02T19:17:04Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-wr9h-g72x-mwhm",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-59425",
              "https://github.com/vllm-project/vllm/commit/ee10d7e6ff5875386c7f136ce8b5f525c8fcef48",
              "https://github.com/vllm-project/vllm",
              "https://github.com/vllm-project/vllm/blob/4b946d693e0af15740e9ca9c0e059d5f333b1083/vllm/entrypoints/openai/api_server.py#L1270-L1274"
            ]
          },
          {
            "id": "GHSA-79j6-g2m3-jgfw",
            "summary": "vLLM has remote code execution vulnerability in the tool call parser for Qwen3-Coder",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-08-21T14:46:51Z",
            "fixed_version": "0.10.1.1",
            "patch_release_date": "2025-08-20T21:20:50Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-79j6-g2m3-jgfw",
              "https://github.com/vllm-project/vllm/pull/21396",
              "https://github.com/vllm-project/vllm/commit/4594fc3b281713bd3d7634405b4a1393af40d294",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-rxc4-3w6r-4v47",
            "summary": "vllm API endpoints vulnerable to Denial of Service Attacks",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-08-21T14:24:16Z",
            "fixed_version": "0.10.1.1",
            "patch_release_date": "2025-08-20T21:20:50Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-rxc4-3w6r-4v47",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-48956",
              "https://github.com/vllm-project/vllm/pull/23267",
              "https://github.com/vllm-project/vllm/commit/d8b736f913a59117803d6701521d2e4861701944",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-9pcc-gvx5-r5wm",
            "summary": "Remote Code Execution Vulnerability in vLLM Multi-Node Cluster Configuration",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-05-06T16:38:35Z",
            "fixed_version": "0.10.0",
            "patch_release_date": "2025-07-24T22:43:46Z",
            "days_to_patch": 79,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-9pcc-gvx5-r5wm",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-30165",
              "https://github.com/vllm-project/vllm",
              "https://github.com/vllm-project/vllm/blob/c21b99b91241409c2fdf9f3f8c542e8748b317be/vllm/distributed/device_communicators/shm_broadcast.py#L295-L301",
              "https://github.com/vllm-project/vllm/blob/c21b99b91241409c2fdf9f3f8c542e8748b317be/vllm/distributed/device_communicators/shm_broadcast.py#L468-L470"
            ]
          },
          {
            "id": "GHSA-9f8f-2vmf-885j",
            "summary": "Data exposure via ZeroMQ on multi-node vLLM deployment",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-04-29T14:50:59Z",
            "fixed_version": "0.8.5",
            "patch_release_date": "2025-04-28T21:13:09Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-9f8f-2vmf-885j",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-30202",
              "https://github.com/vllm-project/vllm/pull/17197",
              "https://github.com/vllm-project/vllm/pull/6183",
              "https://github.com/vllm-project/vllm/commit/a0304dc504c85f421d38ef47c64f83046a13641c"
            ]
          },
          {
            "id": "GHSA-rh4j-5rhw-hr54",
            "summary": "vllm: Malicious model to RCE by torch.load in hf_model_weights_iterator",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2025-01-27T20:50:30Z",
            "fixed_version": "0.7.0",
            "patch_release_date": "2025-01-27T05:50:13Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-rh4j-5rhw-hr54",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-24357",
              "https://github.com/vllm-project/vllm/pull/12366",
              "https://github.com/vllm-project/vllm/commit/d3d6bb13fb62da3234addf6574922a4ec0513d04",
              "https://github.com/pypa/advisory-database/tree/main/vulns/vllm/PYSEC-2025-58.yaml"
            ]
          },
          {
            "id": "GHSA-w2r7-9579-27hf",
            "summary": "vLLM denial of service vulnerability",
            "severity": "HIGH",
            "cvss_score": null,
            "published_date": "2024-09-17T18:33:26Z",
            "fixed_version": "0.5.5",
            "patch_release_date": "2024-08-23T18:37:46Z",
            "days_to_patch": 0,
            "references": [
              "https://nvd.nist.gov/vuln/detail/CVE-2024-8768",
              "https://github.com/vllm-project/vllm/issues/7632",
              "https://github.com/vllm-project/vllm/pull/7746",
              "https://github.com/vllm-project/vllm/commit/e25fee57c2e69161bd261f5986dc5aeb198bbd42",
              "https://access.redhat.com/security/cve/CVE-2024-8768"
            ]
          },
          {
            "id": "GHSA-4qjh-9fv9-r85r",
            "summary": "Potential Timing Side-Channel Vulnerability in vLLM\u2019s Chunk-Based Prefix Caching",
            "severity": "LOW",
            "cvss_score": null,
            "published_date": "2025-05-28T18:02:24Z",
            "fixed_version": "0.9.0",
            "patch_release_date": "2025-05-15T03:38:25Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-4qjh-9fv9-r85r",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-46570",
              "https://github.com/vllm-project/vllm/pull/17045",
              "https://github.com/vllm-project/vllm/commit/77073c77bc2006eb80ea6d5128f076f5e6c6f54f",
              "https://github.com/pypa/advisory-database/tree/main/vulns/vllm/PYSEC-2025-53.yaml"
            ]
          },
          {
            "id": "GHSA-rm76-4mrf-v9r8",
            "summary": "vLLM uses Python 3.12 built-in hash() which leads to predictable hash collisions in prefix cache",
            "severity": "LOW",
            "cvss_score": null,
            "published_date": "2025-02-06T20:00:05Z",
            "fixed_version": "0.7.2",
            "patch_release_date": "2025-02-06T07:30:12Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-rm76-4mrf-v9r8",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-25183",
              "https://github.com/python/cpython/pull/99541",
              "https://github.com/vllm-project/vllm/pull/12621",
              "https://github.com/python/cpython/commit/432117cd1f59c76d97da2eaff55a7d758301dbc7"
            ]
          },
          {
            "id": "GHSA-69j4-grxj-j64p",
            "summary": "vLLM vulnerable to DoS via large Chat Completion or Tokenization requests with specially crafted `chat_template_kwargs`",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-11-20T21:26:24Z",
            "fixed_version": "0.11.1",
            "patch_release_date": "2025-11-18T23:03:42Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-69j4-grxj-j64p",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-62426",
              "https://github.com/vllm-project/vllm/pull/27205",
              "https://github.com/vllm-project/vllm/commit/3ada34f9cb4d1af763fdfa3b481862a93eb6bd2b",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-6fvq-23cw-5628",
            "summary": "vLLM: Resource-Exhaustion (DoS) through Malicious Jinja Template in OpenAI-Compatible Server",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-10-07T21:35:22Z",
            "fixed_version": "0.11.0",
            "patch_release_date": "2025-10-02T19:17:04Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-6fvq-23cw-5628",
              "https://github.com/vllm-project/vllm/pull/25794",
              "https://github.com/vllm-project/vllm/commit/7977e5027c2250a4abc1f474c5619c40b4e5682f",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "PYSEC-2025-54",
            "summary": "vLLM is an inference and serving engine for large language models (LLMs). In versions 0.8.0 up to but excluding 0.9.0, hitting the  /v1/completions API with a invalid json_schema as a Guided Param kil",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-05-30T19:15:30Z",
            "fixed_version": "08bf7840780980c7568c573c70a6a8db94fd45ff",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-6qc9-v4r8-22xg",
              "https://github.com/vllm-project/vllm/commit/08bf7840780980c7568c573c70a6a8db94fd45ff",
              "https://github.com/vllm-project/vllm/pull/17623",
              "https://github.com/vllm-project/vllm/issues/17248"
            ]
          },
          {
            "id": "PYSEC-2025-55",
            "summary": "vLLM is an inference and serving engine for large language models (LLMs). Version 0.8.0 up to but excluding 0.9.0 have a Denial of Service (ReDoS) that causes the vLLM server to crash if an invalid re",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-05-30T19:15:30Z",
            "fixed_version": "08bf7840780980c7568c573c70a6a8db94fd45ff",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-9hcf-v7m4-6m2j",
              "https://github.com/vllm-project/vllm/commit/08bf7840780980c7568c573c70a6a8db94fd45ff",
              "https://github.com/vllm-project/vllm/pull/17623",
              "https://github.com/vllm-project/vllm/issues/17313"
            ]
          },
          {
            "id": "PYSEC-2025-50",
            "summary": "vLLM, an inference and serving engine for large language models (LLMs), has a Regular Expression Denial of Service (ReDoS) vulnerability in the file `vllm/entrypoints/openai/tool_parsers/pythonic_tool",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-05-30T18:15:32Z",
            "fixed_version": "4fc1bf813ad80172c1db31264beaef7d93fe0601",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-w6q7-j642-7c25",
              "https://github.com/vllm-project/vllm/commit/4fc1bf813ad80172c1db31264beaef7d93fe0601",
              "https://github.com/vllm-project/vllm/pull/18454"
            ]
          },
          {
            "id": "PYSEC-2025-43",
            "summary": "vLLM is an inference and serving engine for large language models (LLMs). In versions starting from 0.7.0 to before 0.9.0, in the file vllm/multimodal/hasher.py, the MultiModalHasher class has a secur",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-05-29T17:15:21Z",
            "fixed_version": "99404f53c72965b41558aceb1bc2380875f5d848",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-c65p-x677-fgj6",
              "https://github.com/vllm-project/vllm/commit/99404f53c72965b41558aceb1bc2380875f5d848",
              "https://github.com/vllm-project/vllm/pull/17378"
            ]
          },
          {
            "id": "PYSEC-2025-53",
            "summary": "vLLM is an inference and serving engine for large language models (LLMs). Prior to version 0.9.0, when a new prompt is processed, if the PageAttention mechanism finds a matching prefix chunk, the pref",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-05-29T17:15:21Z",
            "fixed_version": "77073c77bc2006eb80ea6d5128f076f5e6c6f54f",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/pull/17045",
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-4qjh-9fv9-r85r",
              "https://github.com/vllm-project/vllm/commit/77073c77bc2006eb80ea6d5128f076f5e6c6f54f"
            ]
          },
          {
            "id": "GHSA-vrq3-r879-7m65",
            "summary": "vLLM Tool Schema allows DoS via Malformed pattern and type Fields",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-05-28T19:42:32Z",
            "fixed_version": "0.9.0",
            "patch_release_date": "2025-05-15T03:38:25Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-vrq3-r879-7m65",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-48944",
              "https://github.com/vllm-project/vllm/pull/17623",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-9hcf-v7m4-6m2j",
            "summary": "vLLM allows clients to crash the openai server with invalid regex",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-05-28T19:42:12Z",
            "fixed_version": "0.9.0",
            "patch_release_date": "2025-05-15T03:38:25Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-9hcf-v7m4-6m2j",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-48943",
              "https://github.com/vllm-project/vllm/issues/17313",
              "https://github.com/vllm-project/vllm/pull/17623",
              "https://github.com/vllm-project/vllm/commit/08bf7840780980c7568c573c70a6a8db94fd45ff"
            ]
          },
          {
            "id": "GHSA-6qc9-v4r8-22xg",
            "summary": "vLLM DOS: Remotely kill vllm over http with invalid JSON schema",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-05-28T19:41:53Z",
            "fixed_version": "0.9.0",
            "patch_release_date": "2025-05-15T03:38:25Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-6qc9-v4r8-22xg",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-48942",
              "https://github.com/vllm-project/vllm/issues/17248",
              "https://github.com/vllm-project/vllm/pull/17623",
              "https://github.com/vllm-project/vllm/commit/08bf7840780980c7568c573c70a6a8db94fd45ff"
            ]
          },
          {
            "id": "GHSA-c65p-x677-fgj6",
            "summary": "vLLM has a Weakness in MultiModalHasher Image Hashing Implementation",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-05-28T18:03:41Z",
            "fixed_version": "0.9.0",
            "patch_release_date": "2025-05-15T03:38:25Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-c65p-x677-fgj6",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-46722",
              "https://github.com/vllm-project/vllm/pull/17378",
              "https://github.com/vllm-project/vllm/commit/99404f53c72965b41558aceb1bc2380875f5d848",
              "https://github.com/pypa/advisory-database/tree/main/vulns/vllm/PYSEC-2025-43.yaml"
            ]
          },
          {
            "id": "GHSA-j828-28rj-hfhp",
            "summary": "vLLM vulnerable to Regular Expression Denial of Service",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-05-28T17:50:06Z",
            "fixed_version": "0.9.0",
            "patch_release_date": "2025-05-15T03:38:25Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-j828-28rj-hfhp",
              "https://github.com/vllm-project/vllm/pull/18454",
              "https://github.com/vllm-project/vllm/commit/4fc1bf813ad80172c1db31264beaef7d93fe0601",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "GHSA-w6q7-j642-7c25",
            "summary": "vLLM has a Regular Expression Denial of Service (ReDoS, Exponential Complexity) Vulnerability in `pythonic_tool_parser.py`",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-05-28T17:49:33Z",
            "fixed_version": "0.9.0",
            "patch_release_date": "2025-05-15T03:38:25Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-w6q7-j642-7c25",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-48887",
              "https://github.com/vllm-project/vllm/pull/18454",
              "https://github.com/vllm-project/vllm/commit/4fc1bf813ad80172c1db31264beaef7d93fe0601",
              "https://github.com/pypa/advisory-database/tree/main/vulns/vllm/PYSEC-2025-50.yaml"
            ]
          },
          {
            "id": "PYSEC-2025-42",
            "summary": "vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. Versions starting from 0.6.5 and prior to 0.8.5, having vLLM integration with mooncake, are vulnerable to remote c",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-04-30T01:15:51Z",
            "fixed_version": "a5450f11c95847cf51a17207af9a3ca5ab569b2c",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-hj4w-hm2g-p6w5",
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-x3m8-f7g5-qhm7",
              "https://github.com/vllm-project/vllm/commit/a5450f11c95847cf51a17207af9a3ca5ab569b2c",
              "https://github.com/vllm-project/vllm/blob/32b14baf8a1f7195ca09484de3008063569b43c5/vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py#L179"
            ]
          },
          {
            "id": "GHSA-vc6m-hm49-g9qg",
            "summary": "phi4mm: Quadratic Time Complexity in Input Token Processing\u200b leads to denial of service",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-04-29T16:43:10Z",
            "fixed_version": "0.8.5",
            "patch_release_date": "2025-04-28T21:13:09Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-vc6m-hm49-g9qg",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-46560",
              "https://github.com/vllm-project/vllm",
              "https://github.com/vllm-project/vllm/blob/8cac35ba435906fb7eb07e44fe1a8c26e8744f4e/vllm/model_executor/models/phi4mm.py#L1182-L1197"
            ]
          },
          {
            "id": "GHSA-hf3c-wxg2-49q9",
            "summary": "vLLM vulnerable to Denial of Service by abusing xgrammar cache",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-04-15T21:21:04Z",
            "fixed_version": "0.8.4",
            "patch_release_date": "2025-04-14T06:14:20Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/mlc-ai/xgrammar/security/advisories/GHSA-389x-67px-mjg3",
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-hf3c-wxg2-49q9",
              "https://github.com/vllm-project/vllm/pull/16283",
              "https://github.com/vllm-project/vllm/commit/cb84e45ac75b42ba6795145923e8eb323bb825ad",
              "https://github.com/vllm-project/vllm"
            ]
          },
          {
            "id": "PYSEC-2025-63",
            "summary": "vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. When vLLM is configured to use Mooncake, unsafe deserialization exposed directly over ZMQ/TCP on all network inter",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-03-19T16:15:32Z",
            "fixed_version": "288ca110f68d23909728627d3100e5a8db820aa2",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/pull/14228",
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-x3m8-f7g5-qhm7",
              "https://github.com/vllm-project/vllm/commit/288ca110f68d23909728627d3100e5a8db820aa2"
            ]
          },
          {
            "id": "GHSA-mgrm-fgjv-mhv8",
            "summary": "vLLM denial of service via outlines unbounded cache on disk",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2025-03-19T15:52:26Z",
            "fixed_version": "0.8.0",
            "patch_release_date": "2025-03-18T17:52:20Z",
            "days_to_patch": 0,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-mgrm-fgjv-mhv8",
              "https://nvd.nist.gov/vuln/detail/CVE-2025-29770",
              "https://github.com/vllm-project/vllm/pull/14837",
              "https://github.com/vllm-project/vllm",
              "https://github.com/vllm-project/vllm/blob/53be4a863486d02bd96a59c674bbec23eec508f6/vllm/model_executor/guided_decoding/outlines_logits_processors.py"
            ]
          },
          {
            "id": "PYSEC-2025-62",
            "summary": "vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. Maliciously constructed statements can lead to hash collisions, resulting in cache reuse, which can interfere with",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-02-07T20:15:34Z",
            "fixed_version": "432117cd1f59c76d97da2eaff55a7d758301dbc7",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-rm76-4mrf-v9r8",
              "https://github.com/python/cpython/commit/432117cd1f59c76d97da2eaff55a7d758301dbc7",
              "https://github.com/vllm-project/vllm/pull/12621"
            ]
          },
          {
            "id": "PYSEC-2025-58",
            "summary": "vLLM is a library for LLM inference and serving. vllm/model_executor/weight_utils.py implements hf_model_weights_iterator to load the model checkpoint, which is downloaded from huggingface. It uses th",
            "severity": "UNKNOWN",
            "cvss_score": null,
            "published_date": "2025-01-27T18:15:41Z",
            "fixed_version": "d3d6bb13fb62da3234addf6574922a4ec0513d04",
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://github.com/vllm-project/vllm/security/advisories/GHSA-rh4j-5rhw-hr54",
              "https://github.com/vllm-project/vllm/commit/d3d6bb13fb62da3234addf6574922a4ec0513d04",
              "https://github.com/vllm-project/vllm/pull/12366",
              "https://pytorch.org/docs/stable/generated/torch.load.html"
            ]
          },
          {
            "id": "GHSA-wc36-9694-f9rf",
            "summary": "vLLM Denial of Service via the best_of parameter",
            "severity": "MODERATE",
            "cvss_score": null,
            "published_date": "2024-09-17T18:33:26Z",
            "fixed_version": null,
            "patch_release_date": null,
            "days_to_patch": null,
            "references": [
              "https://nvd.nist.gov/vuln/detail/CVE-2024-8939",
              "https://github.com/vllm-project/vllm/issues/6137",
              "https://access.redhat.com/security/cve/CVE-2024-8939",
              "https://bugzilla.redhat.com/show_bug.cgi?id=2312782",
              "https://github.com/vllm-project/vllm"
            ]
          }
        ],
        "avg_days_to_patch": 2.7241379310344827,
        "has_unpatched": true
      }
    },
    "files": {
      "has_readme": true,
      "readme_size_bytes": 4837,
      "has_license": true,
      "has_changelog": false,
      "has_contributing": true,
      "has_code_of_conduct": true,
      "has_codeowners": true,
      "has_governance": false,
      "has_docs_dir": true,
      "has_examples_dir": true,
      "has_tests_dir": true,
      "has_ci_config": true,
      "has_issue_templates": true,
      "has_pr_template": true,
      "has_funding": true
    },
    "ci": {
      "has_github_actions": true,
      "workflow_count": 22,
      "recent_runs_pass_rate": 100.0,
      "has_tests_workflow": true,
      "has_lint_workflow": true,
      "has_security_workflow": true,
      "has_release_workflow": true,
      "has_multi_platform": false
    }
  },
  "llm_assessments": {
    "readme": {
      "clarity": 9,
      "installation": 7,
      "quick_start": 4,
      "examples": 3,
      "configuration": 2,
      "troubleshooting": 2,
      "overall": 5,
      "summary": "Excellent high-level clarity about what vLLM does and its capabilities, but lacks concrete quick-start code examples, configuration guidance, and troubleshooting information in the README itself.",
      "top_issue": "No actual code examples in the README - users must navigate to external documentation to see even basic usage patterns, which creates friction for evaluation and onboarding."
    },
    "sentiment": {
      "sentiment": "mixed",
      "frustration_level": 4,
      "maintainer_responsiveness": "moderate",
      "common_complaints": [
        "Memory management issues (unbounded CPU memory growth with prefix caching)",
        "Performance inefficiencies (redundant quantization kernel launches)",
        "Request handling deadlocks (requests stuck indefinitely)"
      ],
      "praise_themes": [],
      "abandonment_signals": false,
      "summary": "vLLM shows active development with legitimate performance and stability issues being tracked, but recent bugs suggest areas needing attention and maintenance responsiveness appears inconsistent (one issue unanswered for 2+ months, two newer issues with no responses)."
    },
    "maintenance": {
      "status": "abandoned",
      "confidence": 9,
      "concerns": [
        "No commits in past 6 months despite last commit timestamp being recent (suggests data integrity issue or project freeze)",
        "Zero merged PRs in past 6 months indicates no active development",
        "Zero active contributors in past 6 months",
        "No releases in past 6 months",
        "3139 open issues with no recent resolution activity"
      ],
      "positive_signals": [
        "Recent last commit timestamp (2026-01-26) suggests repository still exists and may receive occasional updates"
      ],
      "summary": "The vllm package appears abandoned with zero development activity over the past 6 months, despite a recent timestamp that may indicate stale data or a repository state freeze."
    },
    "llm_provider": "claude",
    "llm_model": "haiku/sonnet",
    "llm_analyzed_at": "2026-01-26T16:44:19.473610+00:00"
  },
  "supply_chain": null,
  "aggregator_data": {
    "scorecard": null,
    "project_metrics": {
      "stars": 66991,
      "forks": 12426,
      "open_issues": 3097,
      "license": "Apache-2.0",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "oss_fuzz_line_count": null,
      "oss_fuzz_line_cover_count": null
    },
    "dependency_graph": {
      "direct_count": 57,
      "transitive_count": 82,
      "vulnerable_direct": 0,
      "vulnerable_transitive": 0,
      "max_depth": 6
    },
    "slsa_attestation": false,
    "slsa_level": null,
    "fetched_at": "2026-01-11T21:18:59.896717Z",
    "sources_available": [
      "deps.dev:version",
      "deps.dev:dependencies",
      "deps.dev:project"
    ]
  },
  "analysis_summary": {
    "maintenance_status": "unknown",
    "security_summary": "42 known CVEs, has SECURITY.md, Dependabot enabled",
    "doc_summary": "",
    "concerns": [],
    "highlights": [
      "Actively maintained",
      "CI/CD configured"
    ],
    "forge_metrics": {
      "stars": 66991,
      "forks": 12426,
      "open_issues": 3097
    },
    "dependency_count": 139
  },
  "analyzed_at": "2026-01-11T21:18:59.897651Z",
  "data_fetched_at": "2026-01-11T21:18:59.897656Z"
}