{
  "ecosystem": "pypi",
  "name": "sageattention",
  "description": "Accurate and efficient 8-bit plug-and-play attention.",
  "version": "1.0.6",
  "homepage": "https://github.com/thu-ml/SageAttention",
  "repository": {
    "platform": "github",
    "owner": "thu-ml",
    "repo": "SageAttention",
    "subpath": null
  },
  "install_count_30d": 134020,
  "data_availability": "available",
  "unavailable_reason": null,
  "scores": {
    "overall": 81.7,
    "grade": "B",
    "percentile": null,
    "risk_tier": "approved",
    "update_urgency": "low",
    "confidence": "medium",
    "confidence_factors": [
      "No LLM assessment available"
    ],
    "project_age_band": "established",
    "security": {
      "score": 80.0,
      "weight": 30,
      "key_factors": []
    },
    "maintenance": {
      "score": 89.94661835033548,
      "weight": 25,
      "key_factors": []
    },
    "community": {
      "score": 100.0,
      "weight": 15,
      "key_factors": []
    },
    "bus_factor": {
      "score": 83.72,
      "weight": 10,
      "key_factors": []
    },
    "documentation": {
      "score": 55.0,
      "weight": 10,
      "key_factors": []
    },
    "stability": {
      "score": 63.0,
      "weight": 10,
      "key_factors": []
    }
  },
  "github_data": {
    "repo": {
      "owner": "thu-ml",
      "name": "SageAttention",
      "description": "[ICLR2025, ICML2025, NeurIPS2025 Spotlight] Quantized Attention achieves speedup of 2-5x compared to FlashAttention, without losing end-to-end metrics across language, image, and video models.",
      "stars": 3042,
      "forks": 312,
      "open_issues": 162,
      "watchers": 3042,
      "created_at": "2024-10-03T09:33:18Z",
      "updated_at": "2026-01-13T12:36:43Z",
      "pushed_at": "2025-12-22T13:15:54Z",
      "default_branch": "main",
      "license": "Apache-2.0",
      "language": "Cuda",
      "topics": [
        "attention",
        "cuda",
        "efficient-attention",
        "inference-acceleration",
        "llm",
        "llm-infra",
        "mlsys",
        "quantization",
        "triton",
        "video-generate",
        "video-generation",
        "vit"
      ],
      "is_archived": false,
      "is_fork": false,
      "has_discussions": false,
      "is_deprecated": false
    },
    "contributors": {
      "total_contributors": 17,
      "active_contributors_6mo": 11,
      "top_contributor_pct": 54.0,
      "contributors_over_5pct": 3,
      "contributors_prev_6mo": 6,
      "contributor_trend": "growing",
      "first_time_contributors_6mo": 10,
      "contributor_entropy": 2.34
    },
    "commits": {
      "last_commit_date": "2025-12-22T13:15:54Z",
      "commits_last_6mo": 72,
      "commits_last_year": 120
    },
    "issues": {
      "open_issues": 146,
      "closed_issues_6mo": 30,
      "avg_response_time_hours": 40.2,
      "avg_close_time_hours": 190.3,
      "good_first_issue_count": 0,
      "regression_issue_count": 0
    },
    "prs": {
      "open_prs": 16,
      "merged_prs_6mo": 16,
      "closed_prs_6mo": 20,
      "stale_prs": 12,
      "avg_merge_time_hours": null
    },
    "releases": {
      "total_releases": 1,
      "releases_last_year": 1,
      "last_release_date": "2025-01-28T06:25:52Z",
      "latest_version": "v2.0.1",
      "has_signed_releases": false,
      "prerelease_ratio": 1.0
    },
    "security": {
      "has_security_md": false,
      "has_security_policy": false,
      "signed_commits_pct": 49.0,
      "has_dependabot": false,
      "has_codeql": false,
      "has_security_ci": false,
      "has_snyk": false,
      "has_renovate": false,
      "has_trivy": false,
      "has_semgrep": false,
      "slsa_level": null,
      "has_sigstore": false,
      "has_sbom": false,
      "has_reproducible_builds": false,
      "known_cves": 0,
      "vulnerable_deps": 0,
      "cve_history": {
        "total_cves": 0,
        "cves": [],
        "avg_days_to_patch": null,
        "has_unpatched": false
      }
    },
    "files": {
      "has_readme": true,
      "readme_size_bytes": 11328,
      "has_license": true,
      "has_changelog": false,
      "has_contributing": false,
      "has_code_of_conduct": false,
      "has_codeowners": false,
      "has_governance": false,
      "has_docs_dir": false,
      "has_examples_dir": true,
      "has_tests_dir": false,
      "has_ci_config": true,
      "has_issue_templates": false,
      "has_pr_template": false,
      "has_funding": false
    },
    "ci": {
      "has_github_actions": true,
      "workflow_count": 1,
      "recent_runs_pass_rate": 50.0,
      "has_tests_workflow": false,
      "has_lint_workflow": false,
      "has_security_workflow": false,
      "has_release_workflow": true,
      "has_multi_platform": false
    }
  },
  "llm_assessments": {},
  "supply_chain": null,
  "aggregator_data": {
    "scorecard": null,
    "project_metrics": {
      "stars": 3037,
      "forks": 310,
      "open_issues": 162,
      "license": "Apache-2.0",
      "description": "[ICLR2025, ICML2025, NeurIPS2025 Spotlight] Quantized Attention achieves speedup of 2-5x compared to FlashAttention, without losing end-to-end metrics across language, image, and video models.",
      "oss_fuzz_line_count": null,
      "oss_fuzz_line_cover_count": null
    },
    "dependency_graph": {
      "direct_count": 0,
      "transitive_count": 0,
      "vulnerable_direct": 0,
      "vulnerable_transitive": 0,
      "max_depth": 0
    },
    "slsa_attestation": false,
    "slsa_level": null,
    "fetched_at": "2026-01-13T20:29:59.110717Z",
    "sources_available": [
      "deps.dev:version",
      "deps.dev:dependencies",
      "deps.dev:project"
    ]
  },
  "analysis_summary": {
    "maintenance_status": "unknown",
    "security_summary": "No issues",
    "doc_summary": "",
    "concerns": [],
    "highlights": [
      "Actively maintained",
      "CI/CD configured"
    ],
    "forge_metrics": {
      "stars": 3037,
      "forks": 310,
      "open_issues": 162
    },
    "dependency_count": 0
  },
  "analyzed_at": "2026-01-13T20:29:59.111246Z",
  "data_fetched_at": "2026-01-13T20:29:59.111248Z"
}